hdoop@sushanth:~/apache-hive-3.1.2-bin/conf$ hive
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/home/hdoop/apache-hive-3.1.2-bin/lib/log4j-slf4j-impl-2.10.0.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/home/hdoop/hadoop-3.2.1/share/hadoop/common/lib/slf4j-log4j12-1.7.25.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.apache.logging.slf4j.Log4jLoggerFactory]
Hive Session ID = 2df1eb88-fd30-4e55-9acd-6573b37c4ceb

Logging initialized using configuration in jar:file:/home/hdoop/apache-hive-3.1.2-bin/lib/hive-common-3.1.2.jar!/hive-log4j2.properties Async: true
Hive Session ID = 16796663-a12a-4b92-b427-aedcb4a0f11a
Hive-on-MR is deprecated in Hive 2 and may not be available in the future versions. Consider using a different execution engine (i.e. spark, tez) or using Hive 1.X releases.
hive> show databases;
OK
customer
default
sushanth
Time taken: 0.855 seconds, Fetched: 3 row(s)
hive> create database employeeDB;
OK
Time taken: 0.228 seconds
hive> use employeeDB;
OK
Time taken: 0.031 seconds
hive> create table employee(name string,ssn int, salary float, address string, dname string, experience int) row format delimited fields terminated by ",";
OK
Time taken: 2.196 seconds
hive> desc Employee;
OK
name                	string              	                    
ssn                 	int                 	                    
salary              	float               	                    
address             	string              	                    
dname               	string              	                    
experience          	int                 	                    
Time taken: 1.766 seconds, Fetched: 6 row(s)
hive> load data local inpath '/HOME/HDOOP/sush.csv' into table employee;
FAILED: SemanticException Line 1:23 Invalid path ''/HOME/HDOOP/sush.csv'': No files matching path file:/HOME/HDOOP/sush.csv
hive> load data local inpath '/home/hdoop/Documents/sush.csv' into table employee;
Loading data to table employeedb.employee
OK
Time taken: 1.213 seconds
hive> select* from employee;
OK
Sushanth	1001	30000.0	Bangalore	ISE	5
Raju	1002	20000.0	Bangalore	ISE	5
Ravi	1003	25000.0	Mangalore	CSE	5
Alexa	1004	80000.0	Udupi	ISE	6
Siri	1005	5000.0	Bihar	ECE	5
Jack	1006	2300.0	Kerala	EEE	4
Pavan	1007	9000.0	Telangana	ISE	7
Hari	1008	15000.0	Ap	ECE	6
Raghav	1009	23500.0	Mangalore	MCE	5
Gagan	1010	6000.0	Bihar	CSE	6
Harshith	1011	41000.0	Tamil nadu	ISE	7
Ali	1012	23650.0	Bangalore	CSE	5
Gaurav	1013	7800.0	Tamil nadu	ISE	6
Anusha	1014	10000.0	Telangana	ISE	5
Namitha	1015	82000.0	Udupi	MCE	5
Reddy	1016	60000.0	Bihar	ECE	5
Suhail	1017	65000.0	Bangalore	ECE	6
Shreyas	1018	32000.0	Mangalore	ISE	7
Reethu	1019	48000.0	Kerala	CSE	8
Krithi	1020	7000.0	Bangalore	ISE	5
Time taken: 2.967 seconds, Fetched: 20 row(s)
hive> insert into employee values("Ram,1021,56000.0,"Mandya","ECE",4),("shyam,1022,59000.0,"Mandya","ECE",6),("Rinku,1023,8000.0,"Ap","ISE",6),("Anu,1024,86000.0,"Mangalore","ECE",5),("Krithi,1025,45000.0,"Kerala","ECE",6);
MismatchedTokenException(24!=374)
	at org.antlr.runtime.BaseRecognizer.recoverFromMismatchedToken(BaseRecognizer.java:617)
	at org.antlr.runtime.BaseRecognizer.match(BaseRecognizer.java:115)
	at org.apache.hadoop.hive.ql.parse.HiveParser_IdentifiersParser.expressionsInParenthesis(HiveParser_IdentifiersParser.java:2316)
	at org.apache.hadoop.hive.ql.parse.HiveParser.expressionsInParenthesis(HiveParser.java:45260)
	at org.apache.hadoop.hive.ql.parse.HiveParser_FromClauseParser.valueRowConstructor(HiveParser_FromClauseParser.java:6214)
	at org.apache.hadoop.hive.ql.parse.HiveParser_FromClauseParser.valuesTableConstructor(HiveParser_FromClauseParser.java:6131)
	at org.apache.hadoop.hive.ql.parse.HiveParser_FromClauseParser.valuesClause(HiveParser_FromClauseParser.java:6045)
	at org.apache.hadoop.hive.ql.parse.HiveParser.valuesClause(HiveParser.java:45342)
	at org.apache.hadoop.hive.ql.parse.HiveParser.regularBody(HiveParser.java:39614)
	at org.apache.hadoop.hive.ql.parse.HiveParser.queryStatementExpressionBody(HiveParser.java:38900)
	at org.apache.hadoop.hive.ql.parse.HiveParser.queryStatementExpression(HiveParser.java:38788)
	at org.apache.hadoop.hive.ql.parse.HiveParser.execStatement(HiveParser.java:2396)
	at org.apache.hadoop.hive.ql.parse.HiveParser.statement(HiveParser.java:1420)
	at org.apache.hadoop.hive.ql.parse.ParseDriver.parse(ParseDriver.java:220)
	at org.apache.hadoop.hive.ql.parse.ParseUtils.parse(ParseUtils.java:74)
	at org.apache.hadoop.hive.ql.parse.ParseUtils.parse(ParseUtils.java:67)
	at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:616)
	at org.apache.hadoop.hive.ql.Driver.compileInternal(Driver.java:1826)
	at org.apache.hadoop.hive.ql.Driver.compileAndRespond(Driver.java:1773)
	at org.apache.hadoop.hive.ql.Driver.compileAndRespond(Driver.java:1768)
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.compileAndRespond(ReExecDriver.java:126)
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:214)
	at org.apache.hadoop.hive.cli.CliDriver.processLocalCmd(CliDriver.java:239)
	at org.apache.hadoop.hive.cli.CliDriver.processCmd(CliDriver.java:188)
	at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:402)
	at org.apache.hadoop.hive.cli.CliDriver.executeDriver(CliDriver.java:821)
	at org.apache.hadoop.hive.cli.CliDriver.run(CliDriver.java:759)
	at org.apache.hadoop.hive.cli.CliDriver.main(CliDriver.java:683)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.util.RunJar.run(RunJar.java:323)
	at org.apache.hadoop.util.RunJar.main(RunJar.java:236)
FAILED: ParseException line 1:47 mismatched input 'Mandya' expecting ) near '"Ram,1021,56000.0,"' in value row constructor
hive> insert into employee values("Ram",1021,56000.0,"Mandya","ECE",4),("shyam",1022,59000.0,"Mandya","ECE",6),("Rinku",1023,8000.0,"Ap","ISE",6),("Anu",1024,86000.0,"Mangalore","ECE",5),("Krithi",1025,45000.0,"Kerala","ECE",6);
Query ID = hdoop_20210705161900_e3f5206e-4130-4df5-b06b-bd8d7bde40ec
Total jobs = 3
Launching Job 1 out of 3
Number of reduce tasks determined at compile time: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>
Starting Job = job_1625480980760_0001, Tracking URL = http://sushanth:8088/proxy/application_1625480980760_0001/
Kill Command = /home/hdoop/hadoop-3.2.1/bin/mapred job  -kill job_1625480980760_0001
Hadoop job information for Stage-1: number of mappers: 1; number of reducers: 1
2021-07-05 16:19:15,512 Stage-1 map = 0%,  reduce = 0%
2021-07-05 16:19:59,383 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 3.22 sec
2021-07-05 16:20:29,203 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 5.5 sec
MapReduce Total cumulative CPU time: 5 seconds 500 msec
Ended Job = job_1625480980760_0001
Stage-4 is selected by condition resolver.
Stage-3 is filtered out by condition resolver.
Stage-5 is filtered out by condition resolver.
Moving data to directory hdfs://127.0.0.1:9000/user/hive/warehouse/employeedb.db/employee/.hive-staging_hive_2021-07-05_16-19-00_459_4237719562725660576-1/-ext-10000
Loading data to table employeedb.employee
MapReduce Jobs Launched: 
Stage-Stage-1: Map: 1  Reduce: 1   Cumulative CPU: 5.5 sec   HDFS Read: 22681 HDFS Write: 647 SUCCESS
Total MapReduce CPU Time Spent: 5 seconds 500 msec
OK
Time taken: 94.761 seconds
hive> select* from employee;
OK
Ram	1021	56000.0	Mandya	ECE	4
shyam	1022	59000.0	Mandya	ECE	6
Rinku	1023	8000.0	Ap	ISE	6
Anu	1024	86000.0	Mangalore	ECE	5
Krithi	1025	45000.0	Kerala	ECE	6
Sushanth	1001	30000.0	Bangalore	ISE	5
Raju	1002	20000.0	Bangalore	ISE	5
Ravi	1003	25000.0	Mangalore	CSE	5
Alexa	1004	80000.0	Udupi	ISE	6
Siri	1005	5000.0	Bihar	ECE	5
Jack	1006	2300.0	Kerala	EEE	4
Pavan	1007	9000.0	Telangana	ISE	7
Hari	1008	15000.0	Ap	ECE	6
Raghav	1009	23500.0	Mangalore	MCE	5
Gagan	1010	6000.0	Bihar	CSE	6
Harshith	1011	41000.0	Tamil nadu	ISE	7
Ali	1012	23650.0	Bangalore	CSE	5
Gaurav	1013	7800.0	Tamil nadu	ISE	6
Anusha	1014	10000.0	Telangana	ISE	5
Namitha	1015	82000.0	Udupi	MCE	5
Reddy	1016	60000.0	Bihar	ECE	5
Suhail	1017	65000.0	Bangalore	ECE	6
Shreyas	1018	32000.0	Mangalore	ISE	7
Reethu	1019	48000.0	Kerala	CSE	8
Krithi	1020	7000.0	Bangalore	ISE	5
Time taken: 0.922 seconds, Fetched: 25 row(s)
hive> show tables;
OK
employee
Time taken: 0.036 seconds, Fetched: 1 row(s)
hive> alter table employee rename to Emp;
OK
Time taken: 0.22 seconds
hive> show tables;
OK
emp
Time taken: 0.036 seconds, Fetched: 1 row(s)
hive> desc emp;
OK
name                	string              	                    
ssn                 	int                 	                    
salary              	float               	                    
address             	string              	                    
dname               	string              	                    
experience          	int                 	                    
Time taken: 0.075 seconds, Fetched: 6 row(s)
hive> alter table emp change dname deptname string;
OK
Time taken: 0.425 seconds
hive> desc emp;
OK
name                	string              	                    
ssn                 	int                 	                    
salary              	float               	                    
address             	string              	                    
deptname            	string              	                    
experience          	int                 	                    
Time taken: 0.075 seconds, Fetched: 6 row(s)
hive> select name,address,experience from emp where address="Bangalore" and experience<5;
OK
Time taken: 1.385 seconds
hive> select name,ssn,salary from emp where salary>=50000;
OK
Ram	1021	56000.0
shyam	1022	59000.0
Anu	1024	86000.0
Alexa	1004	80000.0
Namitha	1015	82000.0
Reddy	1016	60000.0
Suhail	1017	65000.0
Time taken: 1.955 seconds, Fetched: 7 row(s)
hive> create view Emp_Details as select name,deptname from emp;
OK
Time taken: 1.517 seconds
hive> select* from Emp_Details;
OK
Ram	ECE
shyam	ECE
Rinku	ISE
Anu	ECE
Krithi	ECE
Sushanth	ISE
Raju	ISE
Ravi	CSE
Alexa	ISE
Siri	ECE
Jack	EEE
Pavan	ISE
Hari	ECE
Raghav	MCE
Gagan	CSE
Harshith	ISE
Ali	CSE
Gaurav	ISE
Anusha	ISE
Namitha	MCE
Reddy	ECE
Suhail	ECE
Shreyas	ISE
Reethu	CSE
Krithi	ISE
Time taken: 1.768 seconds, Fetched: 25 row(s)
hive> select name,ssn from emp group by name,ssn order by name; 
Query ID = hdoop_20210705163908_71ec3db0-3c84-42b1-a3b4-a5b432057ac6
Total jobs = 2
Launching Job 1 out of 2
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>
Starting Job = job_1625480980760_0002, Tracking URL = http://sushanth:8088/proxy/application_1625480980760_0002/
Kill Command = /home/hdoop/hadoop-3.2.1/bin/mapred job  -kill job_1625480980760_0002
Hadoop job information for Stage-1: number of mappers: 1; number of reducers: 1
2021-07-05 16:39:45,074 Stage-1 map = 0%,  reduce = 0%
2021-07-05 16:40:00,234 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 2.2 sec
2021-07-05 16:40:06,413 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 4.67 sec
MapReduce Total cumulative CPU time: 4 seconds 670 msec
Ended Job = job_1625480980760_0002
Launching Job 2 out of 2
Number of reduce tasks determined at compile time: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>
Starting Job = job_1625480980760_0003, Tracking URL = http://sushanth:8088/proxy/application_1625480980760_0003/
Kill Command = /home/hdoop/hadoop-3.2.1/bin/mapred job  -kill job_1625480980760_0003
Hadoop job information for Stage-2: number of mappers: 1; number of reducers: 1
2021-07-05 16:40:23,195 Stage-2 map = 0%,  reduce = 0%
2021-07-05 16:40:29,387 Stage-2 map = 100%,  reduce = 0%, Cumulative CPU 1.95 sec
2021-07-05 16:40:39,671 Stage-2 map = 100%,  reduce = 100%, Cumulative CPU 4.02 sec
MapReduce Total cumulative CPU time: 4 seconds 20 msec
Ended Job = job_1625480980760_0003
MapReduce Jobs Launched: 
Stage-Stage-1: Map: 1  Reduce: 1   Cumulative CPU: 4.67 sec   HDFS Read: 12724 HDFS Write: 752 SUCCESS
Stage-Stage-2: Map: 1  Reduce: 1   Cumulative CPU: 4.02 sec   HDFS Read: 8162 HDFS Write: 668 SUCCESS
Total MapReduce CPU Time Spent: 8 seconds 690 msec
OK
Alexa	1004
Ali	1012
Anu	1024
Anusha	1014
Gagan	1010
Gaurav	1013
Hari	1008
Harshith	1011
Jack	1006
Krithi	1020
Krithi	1025
Namitha	1015
Pavan	1007
Raghav	1009
Raju	1002
Ram	1021
Ravi	1003
Reddy	1016
Reethu	1019
Rinku	1023
Shreyas	1018
Siri	1005
Suhail	1017
Sushanth	1001
shyam	1022
Time taken: 92.929 seconds, Fetched: 25 row(s)
hive> select max(salary),min(salary),avg(salary) from emp;
Query ID = hdoop_20210705164206_3ca96b06-3169-4155-aebd-d053d06c0b6c
Total jobs = 1
Launching Job 1 out of 1
Number of reduce tasks determined at compile time: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>
Starting Job = job_1625480980760_0004, Tracking URL = http://sushanth:8088/proxy/application_1625480980760_0004/
Kill Command = /home/hdoop/hadoop-3.2.1/bin/mapred job  -kill job_1625480980760_0004
Hadoop job information for Stage-1: number of mappers: 1; number of reducers: 1
2021-07-05 16:42:17,044 Stage-1 map = 0%,  reduce = 0%
2021-07-05 16:42:22,216 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 2.04 sec
2021-07-05 16:42:34,609 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 5.34 sec
MapReduce Total cumulative CPU time: 5 seconds 340 msec
Ended Job = job_1625480980760_0004
MapReduce Jobs Launched: 
Stage-Stage-1: Map: 1  Reduce: 1   Cumulative CPU: 5.34 sec   HDFS Read: 18256 HDFS Write: 122 SUCCESS
Total MapReduce CPU Time Spent: 5 seconds 340 msec
OK
86000.0	2300.0	33850.0
Time taken: 31.206 seconds, Fetched: 1 row(s)
hive> create table department(dno int,dname string)row foramt delimited fields terminated by ",";
NoViableAltException(24@[])
	at org.apache.hadoop.hive.ql.parse.HiveParser.tableRowFormat(HiveParser.java:27979)
	at org.apache.hadoop.hive.ql.parse.HiveParser.createTableStatement(HiveParser.java:6765)
	at org.apache.hadoop.hive.ql.parse.HiveParser.ddlStatement(HiveParser.java:4295)
	at org.apache.hadoop.hive.ql.parse.HiveParser.execStatement(HiveParser.java:2494)
	at org.apache.hadoop.hive.ql.parse.HiveParser.statement(HiveParser.java:1420)
	at org.apache.hadoop.hive.ql.parse.ParseDriver.parse(ParseDriver.java:220)
	at org.apache.hadoop.hive.ql.parse.ParseUtils.parse(ParseUtils.java:74)
	at org.apache.hadoop.hive.ql.parse.ParseUtils.parse(ParseUtils.java:67)
	at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:616)
	at org.apache.hadoop.hive.ql.Driver.compileInternal(Driver.java:1826)
	at org.apache.hadoop.hive.ql.Driver.compileAndRespond(Driver.java:1773)
	at org.apache.hadoop.hive.ql.Driver.compileAndRespond(Driver.java:1768)
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.compileAndRespond(ReExecDriver.java:126)
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:214)
	at org.apache.hadoop.hive.cli.CliDriver.processLocalCmd(CliDriver.java:239)
	at org.apache.hadoop.hive.cli.CliDriver.processCmd(CliDriver.java:188)
	at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:402)
	at org.apache.hadoop.hive.cli.CliDriver.executeDriver(CliDriver.java:821)
	at org.apache.hadoop.hive.cli.CliDriver.run(CliDriver.java:759)
	at org.apache.hadoop.hive.cli.CliDriver.main(CliDriver.java:683)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.util.RunJar.run(RunJar.java:323)
	at org.apache.hadoop.util.RunJar.main(RunJar.java:236)
FAILED: ParseException line 1:49 cannot recognize input near 'row' 'foramt' 'delimited' in table row format specification
hive> create table department(dno int,dname string)row format delimited fields terminated by ",";
OK
Time taken: 2.15 seconds
hive> insert into department values(1,"ISE"),(2,"MCE"),(3,"CSE"),(4,"EEE"),(5,"ECE"), 
    > insert into department values(1,"ISE"),(2,"MCE"),(3,"CSE"),(4,"EEE"),(5,"ECE"); 
MismatchedTokenException(158!=361)
	at org.antlr.runtime.BaseRecognizer.recoverFromMismatchedToken(BaseRecognizer.java:617)
	at org.antlr.runtime.BaseRecognizer.match(BaseRecognizer.java:115)
	at org.apache.hadoop.hive.ql.parse.HiveParser_IdentifiersParser.expressionsInParenthesis(HiveParser_IdentifiersParser.java:2309)
	at org.apache.hadoop.hive.ql.parse.HiveParser.expressionsInParenthesis(HiveParser.java:45260)
	at org.apache.hadoop.hive.ql.parse.HiveParser_FromClauseParser.valueRowConstructor(HiveParser_FromClauseParser.java:6214)
	at org.apache.hadoop.hive.ql.parse.HiveParser_FromClauseParser.valuesTableConstructor(HiveParser_FromClauseParser.java:6151)
	at org.apache.hadoop.hive.ql.parse.HiveParser_FromClauseParser.valuesClause(HiveParser_FromClauseParser.java:6045)
	at org.apache.hadoop.hive.ql.parse.HiveParser.valuesClause(HiveParser.java:45342)
	at org.apache.hadoop.hive.ql.parse.HiveParser.regularBody(HiveParser.java:39614)
	at org.apache.hadoop.hive.ql.parse.HiveParser.queryStatementExpressionBody(HiveParser.java:38900)
	at org.apache.hadoop.hive.ql.parse.HiveParser.queryStatementExpression(HiveParser.java:38788)
	at org.apache.hadoop.hive.ql.parse.HiveParser.execStatement(HiveParser.java:2396)
	at org.apache.hadoop.hive.ql.parse.HiveParser.statement(HiveParser.java:1420)
	at org.apache.hadoop.hive.ql.parse.ParseDriver.parse(ParseDriver.java:220)
	at org.apache.hadoop.hive.ql.parse.ParseUtils.parse(ParseUtils.java:74)
	at org.apache.hadoop.hive.ql.parse.ParseUtils.parse(ParseUtils.java:67)
	at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:616)
	at org.apache.hadoop.hive.ql.Driver.compileInternal(Driver.java:1826)
	at org.apache.hadoop.hive.ql.Driver.compileAndRespond(Driver.java:1773)
	at org.apache.hadoop.hive.ql.Driver.compileAndRespond(Driver.java:1768)
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.compileAndRespond(ReExecDriver.java:126)
	at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:214)
	at org.apache.hadoop.hive.cli.CliDriver.processLocalCmd(CliDriver.java:239)
	at org.apache.hadoop.hive.cli.CliDriver.processCmd(CliDriver.java:188)
	at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:402)
	at org.apache.hadoop.hive.cli.CliDriver.executeDriver(CliDriver.java:821)
	at org.apache.hadoop.hive.cli.CliDriver.run(CliDriver.java:759)
	at org.apache.hadoop.hive.cli.CliDriver.main(CliDriver.java:683)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.util.RunJar.run(RunJar.java:323)
	at org.apache.hadoop.util.RunJar.main(RunJar.java:236)
FAILED: ParseException line 2:0 mismatched input 'insert' expecting ( near ',' in value row constructor
hive> insert into department values(1,"ISE"),(2,"MCE"),(3,"CSE"),(4,"EEE"),(5,"ECE"); 
Query ID = hdoop_20210705164819_4b12000d-059b-4cc4-9d8b-a4d5fc52a0a7
Total jobs = 3
Launching Job 1 out of 3
Number of reduce tasks determined at compile time: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>
Starting Job = job_1625480980760_0005, Tracking URL = http://sushanth:8088/proxy/application_1625480980760_0005/
Kill Command = /home/hdoop/hadoop-3.2.1/bin/mapred job  -kill job_1625480980760_0005
Hadoop job information for Stage-1: number of mappers: 1; number of reducers: 1
2021-07-05 16:48:32,179 Stage-1 map = 0%,  reduce = 0%
2021-07-05 16:48:57,871 Stage-1 map = 67%,  reduce = 0%, Cumulative CPU 15.93 sec
2021-07-05 16:49:03,009 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 16.13 sec
2021-07-05 16:49:21,504 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 18.62 sec
MapReduce Total cumulative CPU time: 18 seconds 620 msec
Ended Job = job_1625480980760_0005
Stage-4 is selected by condition resolver.
Stage-3 is filtered out by condition resolver.
Stage-5 is filtered out by condition resolver.
Moving data to directory hdfs://127.0.0.1:9000/user/hive/warehouse/employeedb.db/department/.hive-staging_hive_2021-07-05_16-48-19_820_8447158862064138451-1/-ext-10000
Loading data to table employeedb.department
MapReduce Jobs Launched: 
Stage-Stage-1: Map: 1  Reduce: 1   Cumulative CPU: 18.62 sec   HDFS Read: 15804 HDFS Write: 310 SUCCESS
Total MapReduce CPU Time Spent: 18 seconds 620 msec
OK
Time taken: 71.251 seconds
hive> select* from department;
OK
1	ISE
2	MCE
3	CSE
4	EEE
5	ECE
Time taken: 0.682 seconds, Fetched: 5 row(s)
hive> select name,ssn,d.deptname,dno from emp e full outer join department d on e.deptname=d.deptname;
FAILED: SemanticException [Error 10002]: Line 1:87 Invalid column reference 'deptname'
hive> desc emp;
OK
name                	string              	                    
ssn                 	int                 	                    
salary              	float               	                    
address             	string              	                    
deptname            	string              	                    
experience          	int                 	                    
Time taken: 1.608 seconds, Fetched: 6 row(s)
hive> desc department;
OK
dno                 	int                 	                    
dname               	string              	                    
Time taken: 0.044 seconds, Fetched: 2 row(s)
hive> select name,ssn,d.deptname,dno from emp e full outer join department d on e.deptname=d.dname;
FAILED: SemanticException [Error 10002]: Line 1:18 Invalid column reference 'deptname'
hive> select name,ssn,d.dname,dno from emp e full outer join department d on e.deptname=d.dname;
Query ID = hdoop_20210705165436_17bd72dc-bde1-4f07-a738-7a26ebab3e03
Total jobs = 1
Launching Job 1 out of 1
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>
Starting Job = job_1625480980760_0006, Tracking URL = http://sushanth:8088/proxy/application_1625480980760_0006/
Kill Command = /home/hdoop/hadoop-3.2.1/bin/mapred job  -kill job_1625480980760_0006
Hadoop job information for Stage-1: number of mappers: 2; number of reducers: 1
2021-07-05 16:55:11,286 Stage-1 map = 0%,  reduce = 0%
2021-07-05 16:55:21,819 Stage-1 map = 50%,  reduce = 0%, Cumulative CPU 4.96 sec
2021-07-05 16:55:32,087 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 10.72 sec
2021-07-05 16:55:44,451 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 13.3 sec
MapReduce Total cumulative CPU time: 13 seconds 300 msec
Ended Job = job_1625480980760_0006
MapReduce Jobs Launched: 
Stage-Stage-1: Map: 2  Reduce: 1   Cumulative CPU: 13.3 sec   HDFS Read: 17669 HDFS Write: 818 SUCCESS
Total MapReduce CPU Time Spent: 13 seconds 300 msec
OK
Ravi	1003	CSE	3
Gagan	1010	CSE	3
Ali	1012	CSE	3
Reethu	1019	CSE	3
Ram	1021	ECE	5
Suhail	1017	ECE	5
Reddy	1016	ECE	5
Hari	1008	ECE	5
Siri	1005	ECE	5
Krithi	1025	ECE	5
Anu	1024	ECE	5
shyam	1022	ECE	5
Jack	1006	EEE	4
Rinku	1023	ISE	1
Pavan	1007	ISE	1
Krithi	1020	ISE	1
Alexa	1004	ISE	1
Shreyas	1018	ISE	1
Raju	1002	ISE	1
Sushanth	1001	ISE	1
Gaurav	1013	ISE	1
Anusha	1014	ISE	1
Harshith	1011	ISE	1
Namitha	1015	MCE	2
Raghav	1009	MCE	2
Time taken: 75.704 seconds, Fetched: 25 row(s)
hive> select name ssn,d.dname,dno from emp e left outer join department d on e.deptname=d.dname;
Query ID = hdoop_20210705165730_6fcc2b97-9ae9-4db9-a6af-17c1d775753c
Total jobs = 1
SLF4J: Found binding in [jar:file:/home/hdoop/apache-hive-3.1.2-bin/lib/log4j-slf4j-impl-2.10.0.jar!/org/slf4j/impl/StaticLoggerBinder.class]
2021-07-05 17:00:04	Starting to launch local task to process map join;	maximum memory = 239075328

2021-07-05 17:00:17	Uploaded 1 File to: file:/tmp/hive/java/hdoop/2df1eb88-fd30-4e55-9acd-6573b37c4ceb/hive_2021-07-05_16-57-30_543_8296294841225471837-1/-local-10004/HashTable-Stage-3/MapJoin-mapfile01--.hashtable (375 bytes)
2021-07-05 17:00:17	End of local task; Time Taken: 13.408 sec.
Execution completed successfully
MapredLocal task succeeded
Launching Job 1 out of 1
Number of reduce tasks is set to 0 since there's no reduce operator
Starting Job = job_1625480980760_0007, Tracking URL = http://sushanth:8088/proxy/application_1625480980760_0007/
Kill Command = /home/hdoop/hadoop-3.2.1/bin/mapred job  -kill job_1625480980760_0007
Hadoop job information for Stage-3: number of mappers: 1; number of reducers: 0
2021-07-05 17:00:40,088 Stage-3 map = 0%,  reduce = 0%
2021-07-05 17:00:52,438 Stage-3 map = 100%,  reduce = 0%, Cumulative CPU 2.27 sec
MapReduce Total cumulative CPU time: 2 seconds 270 msec
Ended Job = job_1625480980760_0007
MapReduce Jobs Launched: 
Stage-Stage-3: Map: 1   Cumulative CPU: 2.27 sec   HDFS Read: 10091 HDFS Write: 693 SUCCESS
Total MapReduce CPU Time Spent: 2 seconds 270 msec
OK
Ram	ECE	5
shyam	ECE	5
Rinku	ISE	1
Anu	ECE	5
Krithi	ECE	5
Sushanth	ISE	1
Raju	ISE	1
Ravi	CSE	3
Alexa	ISE	1
Siri	ECE	5
Jack	EEE	4
Pavan	ISE	1
Hari	ECE	5
Raghav	MCE	2
Gagan	CSE	3
Harshith	ISE	1
Ali	CSE	3
Gaurav	ISE	1
Anusha	ISE	1
Namitha	MCE	2
Reddy	ECE	5
Suhail	ECE	5
Shreyas	ISE	1
Reethu	CSE	3
Krithi	ISE	1
Time taken: 206.286 seconds, Fetched: 25 row(s)
hive> select name,ssn,d.dname,dno from emp e right outer join department d on e.deptname = d.dname;
Query ID = hdoop_20210705170351_1b0d1ca5-435e-4c5c-9404-69d95c381ed5
Total jobs = 1

2021-07-05 17:03:57	Starting to launch local task to process map join;	maximum memory = 239075328
2021-07-05 17:03:59	Uploaded 1 File to: file:/tmp/hive/java/hdoop/2df1eb88-fd30-4e55-9acd-6573b37c4ceb/hive_2021-07-05_17-03-51_623_3353144385287407466-1/-local-10004/HashTable-Stage-3/MapJoin-mapfile10--.hashtable (704 bytes)
Execution completed successfully
MapredLocal task succeeded
Launching Job 1 out of 1
Number of reduce tasks is set to 0 since there's no reduce operator
Starting Job = job_1625480980760_0008, Tracking URL = http://sushanth:8088/proxy/application_1625480980760_0008/
Kill Command = /home/hdoop/hadoop-3.2.1/bin/mapred job  -kill job_1625480980760_0008
Hadoop job information for Stage-3: number of mappers: 1; number of reducers: 0
2021-07-05 17:04:17,125 Stage-3 map = 0%,  reduce = 0%
2021-07-05 17:04:23,314 Stage-3 map = 100%,  reduce = 0%, Cumulative CPU 2.18 sec
MapReduce Total cumulative CPU time: 2 seconds 180 msec
Ended Job = job_1625480980760_0008
MapReduce Jobs Launched: 
Stage-Stage-3: Map: 1   Cumulative CPU: 2.18 sec   HDFS Read: 9031 HDFS Write: 818 SUCCESS
Total MapReduce CPU Time Spent: 2 seconds 180 msec
OK
Rinku	1023	ISE	1
Sushanth	1001	ISE	1
Raju	1002	ISE	1
Alexa	1004	ISE	1
Pavan	1007	ISE	1
Harshith	1011	ISE	1
Gaurav	1013	ISE	1
Anusha	1014	ISE	1
Shreyas	1018	ISE	1
Krithi	1020	ISE	1
Raghav	1009	MCE	2
Namitha	1015	MCE	2
Ravi	1003	CSE	3
Gagan	1010	CSE	3
Ali	1012	CSE	3
Reethu	1019	CSE	3
Jack	1006	EEE	4
Ram	1021	ECE	5
shyam	1022	ECE	5
Anu	1024	ECE	5
Krithi	1025	ECE	5
Siri	1005	ECE	5
Hari	1008	ECE	5
Reddy	1016	ECE	5
Suhail	1017	ECE	5
Time taken: 37.03 seconds, Fetched: 25 row(s)
hive> 

