hive> show databases;
OK
default
sushanth
Time taken: 0.017 seconds, Fetched: 2 row(s)
hive> create database customer;
OK
Time taken: 0.359 seconds
hive> use customer;
OK
Time taken: 0.026 seconds

hive> create table bank(tid int , bname String, cname String,amount double)row format delimited fields terminated by ',';
OK
Time taken: 0.412 seconds
hive> insert into bank values(101,"axis","sushanth",20000),(102,"icici","raju",25000),(103,"hdfc","rocky",2000),(104,"canara","ravi",28000),(105,"yesbank","alex",10000);
Query ID = hdoop_20210704192114_84c39d4b-bc61-42e9-a9d1-35ae416da5d2
Total jobs = 3
Launching Job 1 out of 3
Number of reduce tasks determined at compile time: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>
Starting Job = job_1625392123150_0014, Tracking URL = http://sushanth:8088/proxy/application_1625392123150_0014/
Kill Command = /home/hdoop/hadoop-3.2.1/bin/mapred job  -kill job_1625392123150_0014
Hadoop job information for Stage-1: number of mappers: 1; number of reducers: 1
2021-07-04 19:21:22,647 Stage-1 map = 0%,  reduce = 0%
2021-07-04 19:21:28,849 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 3.03 sec
2021-07-04 19:21:35,094 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 5.26 sec
MapReduce Total cumulative CPU time: 5 seconds 260 msec
Ended Job = job_1625392123150_0014
Stage-4 is selected by condition resolver.
Stage-3 is filtered out by condition resolver.
Stage-5 is filtered out by condition resolver.
Moving data to directory hdfs://127.0.0.1:9000/user/hive/warehouse/customer.db/bank/.hive-staging_hive_2021-07-04_19-21-14_565_462480762374686861-1/-ext-10000
Loading data to table customer.bank
MapReduce Jobs Launched: 
Stage-Stage-1: Map: 1  Reduce: 1   Cumulative CPU: 5.26 sec   HDFS Read: 19346 HDFS Write: 531 SUCCESS
Total MapReduce CPU Time Spent: 5 seconds 260 msec
OK
Time taken: 22.965 seconds

hive> select* from bank;
OK
101	axis	sushanth	20000.0
102	icici	raju	25000.0
103	hdfc	rocky	2000.0
104	canara	ravi	28000.0
105	yesbank	alex	10000.0
Time taken: 2.899 seconds, Fetched: 5 row(s)
hive> load data local inpath '/home/hdoop/customer.csv' into table bank;
Loading data to table customer.bank
OK
Time taken: 0.982 seconds
hive> select* from bank;
OK
101	axis	sushanth	20000.0
102	icici	raju	25000.0
103	hdfc	rocky	2000.0
104	canara	ravi	28000.0
105	yesbank	alex	10000.0
506	"bob"	"siri"	35000.0
507	"pnb"	"gagan"	40000.0
508	"visa"	"aadarsh"	50000.0
Time taken: 0.172 seconds, Fetched: 8 row(s)
hive> select cname,amount from bank where amount >=30000.00 group by cname,amount;
Query ID = hdoop_20210704192350_a9472207-1222-40b7-8b81-9afc305385f3
Total jobs = 1
Launching Job 1 out of 1
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>
Starting Job = job_1625392123150_0015, Tracking URL = http://sushanth:8088/proxy/application_1625392123150_0015/
Kill Command = /home/hdoop/hadoop-3.2.1/bin/mapred job  -kill job_1625392123150_0015
Hadoop job information for Stage-1: number of mappers: 1; number of reducers: 1
2021-07-04 19:24:00,160 Stage-1 map = 0%,  reduce = 0%
2021-07-04 19:24:05,454 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 2.63 sec
2021-07-04 19:24:11,753 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 5.43 sec
MapReduce Total cumulative CPU time: 5 seconds 430 msec
Ended Job = job_1625392123150_0015
MapReduce Jobs Launched: 
Stage-Stage-1: Map: 1  Reduce: 1   Cumulative CPU: 5.43 sec   HDFS Read: 13077 HDFS Write: 172 SUCCESS
Total MapReduce CPU Time Spent: 5 seconds 430 msec
OK
"aadarsh"	50000.0
"gagan"	40000.0
"siri"	35000.0
Time taken: 23.516 seconds, Fetched: 3 row(s)
hive> select max(amount) as Maxamt, min(amount) as Minamt, avg(amount) as Avgamt from bank;
Query ID = hdoop_20210704192818_292097c5-8ca1-46b7-9711-eaf1f8d0256a
Total jobs = 1
Launching Job 1 out of 1
Number of reduce tasks determined at compile time: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>
Starting Job = job_1625392123150_0016, Tracking URL = http://sushanth:8088/proxy/application_1625392123150_0016/
Kill Command = /home/hdoop/hadoop-3.2.1/bin/mapred job  -kill job_1625392123150_0016
Hadoop job information for Stage-1: number of mappers: 1; number of reducers: 1
2021-07-04 19:28:26,544 Stage-1 map = 0%,  reduce = 0%
2021-07-04 19:28:31,736 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 2.2 sec
2021-07-04 19:28:38,966 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 5.13 sec
MapReduce Total cumulative CPU time: 5 seconds 130 msec
Ended Job = job_1625392123150_0016
MapReduce Jobs Launched: 
Stage-Stage-1: Map: 1  Reduce: 1   Cumulative CPU: 5.13 sec   HDFS Read: 17049 HDFS Write: 122 SUCCESS
Total MapReduce CPU Time Spent: 5 seconds 130 msec
OK
50000.0	2000.0	26250.0
Time taken: 21.878 seconds, Fetched: 1 row(s)
hive> select tid,cname,amount from bank where amount>25000 order by amount DESC;
Query ID = hdoop_20210704192956_3bd93477-f183-4966-adf0-fc8dd31cdade
Total jobs = 1
Launching Job 1 out of 1
Number of reduce tasks determined at compile time: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>
Starting Job = job_1625392123150_0017, Tracking URL = http://sushanth:8088/proxy/application_1625392123150_0017/
Kill Command = /home/hdoop/hadoop-3.2.1/bin/mapred job  -kill job_1625392123150_0017
Hadoop job information for Stage-1: number of mappers: 1; number of reducers: 1
2021-07-04 19:30:03,016 Stage-1 map = 0%,  reduce = 0%
2021-07-04 19:30:09,221 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 2.76 sec
2021-07-04 19:30:14,418 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 5.42 sec
MapReduce Total cumulative CPU time: 5 seconds 420 msec
Ended Job = job_1625392123150_0017
MapReduce Jobs Launched: 
Stage-Stage-1: Map: 1  Reduce: 1   Cumulative CPU: 5.42 sec   HDFS Read: 12271 HDFS Write: 213 SUCCESS
Total MapReduce CPU Time Spent: 5 seconds 420 msec
OK
508	"aadarsh"	50000.0
507	"gagan"	40000.0
506	"siri"	35000.0
104	ravi	28000.0
Time taken: 19.416 seconds, Fetched: 4 row(s)

hive> select tid,cname,sum(amount) from bank group by tid,cname having sum(amount)>40000;
Query ID = hdoop_20210704193146_993de3d5-a446-4a93-a7d0-ec6df4da2241
Total jobs = 1
Launching Job 1 out of 1
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>
Starting Job = job_1625392123150_0018, Tracking URL = http://sushanth:8088/proxy/application_1625392123150_0018/
Kill Command = /home/hdoop/hadoop-3.2.1/bin/mapred job  -kill job_1625392123150_0018
Hadoop job information for Stage-1: number of mappers: 1; number of reducers: 1
2021-07-04 19:31:52,963 Stage-1 map = 0%,  reduce = 0%
2021-07-04 19:31:58,151 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 1.93 sec
2021-07-04 19:32:05,359 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 5.17 sec
MapReduce Total cumulative CPU time: 5 seconds 170 msec
Ended Job = job_1625392123150_0018
MapReduce Jobs Launched: 
Stage-Stage-1: Map: 1  Reduce: 1   Cumulative CPU: 5.17 sec   HDFS Read: 14638 HDFS Write: 121 SUCCESS
Total MapReduce CPU Time Spent: 5 seconds 170 msec
OK
508	"aadarsh"	50000.0
Time taken: 20.398 seconds, Fetched: 1 row(s)
hive> create view Cust_Transac as select tid,cname from bank;
OK
Time taken: 0.311 seconds
hive> select* from Cust_Transac;
OK
101	sushanth
102	raju
103	rocky
104	ravi
105	alex
506	"siri"
507	"gagan"
508	"aadarsh"
Time taken: 0.328 seconds, Fetched: 8 row(s)
hive> create table customer(cust_id int, tid int, loc string)row format delimited fields terminated by ',';
OK
Time taken: 0.093 seconds
 
hive> use customer;
OK
Time taken: 0.035 seconds
hive> desc customer;
OK
cust_id             	int                 	                    
tid                 	int                 	                    
loc                 	string              	                    
Time taken: 0.078 seconds, Fetched: 3 row(s)
hive> load data local inpath '/home/hdoop/data.csv' into table customer;
Loading data to table customer.customer
OK
Time taken: 0.292 seconds
hive> select* from customer;
OK
101	501	'Mangalore'
102	502	'Bangalore'
103	503	'Kerala'
104	504	'bihar'
Time taken: 0.188 seconds, Fetched: 4 row(s)
hive> select cname,loc from bank b , customer c where b.tid=c.tid;
Query ID = hdoop_20210704194136_1004f409-98b6-457d-b71c-aca4928f0db3
Total jobs = 1
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
2021-07-04 19:41:43	Starting to launch local task to process map join;	maximum memory = 239075328
2021-07-04 19:41:44	Dump the side-table for tag: 1 with group count: 4 into file: file:/tmp/hive/java/hdoop/eb3cb311-36da-40a9-bc84-375f0a6ece3f/hive_2021-07-04_19-41-36_368_7516904462225091515-1/-local-10004/HashTable-Stage-3/MapJoin-mapfile01--.hashtable
Execution completed successfully
MapredLocal task succeeded
Launching Job 1 out of 1
Number of reduce tasks is set to 0 since there's no reduce operator
Starting Job = job_1625392123150_0019, Tracking URL = http://sushanth:8088/proxy/application_1625392123150_0019/
Kill Command = /home/hdoop/hadoop-3.2.1/bin/mapred job  -kill job_1625392123150_0019
Hadoop job information for Stage-3: number of mappers: 1; number of reducers: 0
2021-07-04 19:41:54,974 Stage-3 map = 0%,  reduce = 0%
2021-07-04 19:42:01,271 Stage-3 map = 100%,  reduce = 0%, Cumulative CPU 2.54 sec
MapReduce Total cumulative CPU time: 2 seconds 540 msec
Ended Job = job_1625392123150_0019
MapReduce Jobs Launched: 
Stage-Stage-3: Map: 1   Cumulative CPU: 2.54 sec   HDFS Read: 9418 HDFS Write: 87 SUCCESS
Total MapReduce CPU Time Spent: 2 seconds 540 msec
OK
Time taken: 26.079 seconds
hive> alter table customer rename to cust;
OK
Time taken: 0.196 seconds
hive> show tables;
OK
bank
cust
cust_transac
Time taken: 0.028 seconds, Fetched: 3 row(s)

hive> alter table cust add columns(phno int);
OK
Time taken: 0.227 seconds
hive> desc cust;
OK
cust_id             	int                 	                    
tid                 	int                 	                    
loc                 	string              	                    
phno                	int                 	                    
Time taken: 0.073 seconds, Fetched: 4 row(s)
hive> insert into cust values(105,505,"ap",906516065);
Query ID = hdoop_20210704194441_136dc340-12b7-4093-a2b1-24f64fd3c220
Total jobs = 3
Launching Job 1 out of 3
Number of reduce tasks determined at compile time: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>
Starting Job = job_1625392123150_0020, Tracking URL = http://sushanth:8088/proxy/application_1625392123150_0020/
Kill Command = /home/hdoop/hadoop-3.2.1/bin/mapred job  -kill job_1625392123150_0020
Hadoop job information for Stage-1: number of mappers: 1; number of reducers: 1
2021-07-04 19:44:49,076 Stage-1 map = 0%,  reduce = 0%
2021-07-04 19:44:56,338 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 2.94 sec
2021-07-04 19:45:02,570 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 4.98 sec
MapReduce Total cumulative CPU time: 4 seconds 980 msec
Ended Job = job_1625392123150_0020
Stage-4 is selected by condition resolver.
Stage-3 is filtered out by condition resolver.
Stage-5 is filtered out by condition resolver.
Moving data to directory hdfs://127.0.0.1:9000/user/hive/warehouse/customer.db/cust/.hive-staging_hive_2021-07-04_19-44-41_295_5804126970965971997-1/-ext-10000
Loading data to table customer.cust
MapReduce Jobs Launched: 
Stage-Stage-1: Map: 1  Reduce: 1   Cumulative CPU: 4.98 sec   HDFS Read: 17490 HDFS Write: 338 SUCCESS
Total MapReduce CPU Time Spent: 4 seconds 980 msec
OK
Time taken: 22.747 seconds
hive> select* from cust;
OK
105	505	ap	906516065
101	501	'Mangalore'	NULL
102	502	'Bangalore'	NULL
103	503	'Kerala'	NULL
104	504	'bihar'	NULL
Time taken: 0.163 seconds, Fetched: 5 row(s)
hive> 

